# Sprint3

- Topic : PEFT, Quantization, Efficient Transformer, Knowledge Distillation, AI Agent
- Period : 2024.01.24 ~ 2024.03.27(TBD)
- Presenter : 강동규, 오원준, 나보영, 이민지, 이승호, 김민준, 김해문, 박지영

| Date | Paper | Category | Presenter | Link |
| ---- | ---- | ---- | ---- | ---- |
| 2024.01.24 | Parameter-Efficient Transfer Learning for NLP | PEFT | 나보영 | [Paper](https://arxiv.org/abs/1902.00751)</br>[Review](https://github.com/devkade/DeepSync/blob/main/Docs/sprint3/Parameter-Efficient%20Transfer%20Learning%20for%20NLP.pdf) |
| 2024.01.31 | LoRA | PEFT | 강동규 | [Paper](https://arxiv.org/abs/2106.09685)</br> [Slide](https://github.com/devkade/DeepSync/tree/main/Docs/sprint3/LoRA.pdf) |
| 2024.02.14 | Prefix-Tuning | PEFT | 박지영 | [Paper](https://arxiv.org/abs/2101.00190) |
| 2024.02.21 | Few-Shot Parameter-Efficient Fine-Tuning vs. In-Context Learning | PEFT | 강동규 | [Paper](https://arxiv.org/abs/2205.05638) |
|  | QLoRA | PEFT |  | [Paper](https://arxiv.org/abs/2305.14314) |
|  | A Survey of Quantization Methods for Efficient Neural Network Inference | Quantization |  | [Paper](https://arxiv.org/abs/2103.13630) |
|  | Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference | Quantization |  | [Paper](https://arxiv.org/abs/1712.05877) |
| 2024.02.14 | Mixed Precision Training | Quantization | 이민지 | [Paper](https://arxiv.org/abs/1710.03740) |
|  | Quant-Noise | Quantization |  | [Paper](https://arxiv.org/abs/2004.07320) |
|  | LLM.int8() | Quantization |  | [Paper](https://arxiv.org/abs/2208.07339) |
|  | GPTQ | Quantization |  | [Paper](https://arxiv.org/abs/2210.17323) |
| 2024.01.24 | Transformer 톺아보기 | Efficient Transformer | 이승호 |[Review](https://github.com/devkade/DeepSync/blob/main/Docs/sprint3/Transformer%ED%86%BA%EC%95%84%EB%B3%B4%EA%B8%B0.md)  |
| 2024.02.14 | MegatronLM | Efficient Transformer | 이승호 | [Paper](https://arxiv.org/abs/1909.08053) |
| 2024.01.24 | Swin Transformer | Efficient Transformer | 오원준 | [Paper](https://arxiv.org/abs/2103.14030)</br>[Review](https://ownogatari.xyz/posts/swin/)</br>[Slides](https://drive.google.com/file/d/1lKkg_PGO7e2EJt5hMZuLiPtA4QZJhIgo/view?usp=sharing)   |
|  | zero | Efficient Transformer |  | [Paper](https://arxiv.org/abs/1910.02054) |
|  | zero infinity | Efficient Transformer |  | [Paper](https://arxiv.org/abs/2104.07857) |
| 2024.02.14 | Switch Transformer | Efficient Transformer | 오원준  | [Paper](https://arxiv.org/abs/2101.03961) |
|  | DeepSpeed-MoE or Extreme Compression for Pre-trained Transformers Made Simple and Efficient | Efficient Transformer |  | [Paper](https://arxiv.org/abs/2201.05596) |
| 2024.01.31 | Paged Attention | Efficient Transformer | 김민준 | [Paper](https://arxiv.org/abs/2309.06180) |
|  | FlashAttention | Efficient Transformer |  | [Paper](https://arxiv.org/abs/2205.14135) |
|  | FastViT | Efficient Transformer |  | [Paper](https://arxiv.org/pdf/2303.14189.pdf) |
|  | Biformer | Efficient Transformer |  | [Paper](https://arxiv.org/pdf/2303.08810.pdf) |
|  | Knowledge Distillation: A Survey | Knowledge Distillation |  | [Paper](https://arxiv.org/abs/2006.05525) |
| 2024.02.07 | DeiT | Knowledge Distillation | 김해문 | [Paper](https://arxiv.org/abs/2012.12877) |
| 2024.01.31 | Generative Agents: Interactive Simulacra of Human Behavior | AI Agent | 박지영 | [Paper](https://arxiv.org/abs/2304.03442)</br>[Review](https://github.com/devkade/DeepSync/blob/main/Docs/sprint3/Generative.Agents-Interactive.Simulacra.of.Human.Behavior.pdf) |
|  | The Rise and Potential of Large Language Model Based Agents: A Survey | AI Agent |  | [Paper](https://arxiv.org/abs/2309.07864) |
| 2024.02.07 | Agents: An Open-source Framework for Autonomous Language Agents | AI Agent | 나보영 | [Paper](https://arxiv.org/abs/2309.07870)</br>      |
|  | JARVIS-1: Open-World Multi-task Agents with Memory-Augmented Multimodal Language Models | AI Agent |  | [Paper](https://arxiv.org/abs/2311.05997) |
