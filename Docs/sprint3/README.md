# Sprint3

- Topic : PEFT, Quantization, Efficient Transformer, Knowledge Distillation, AI Agent
- Period : 2024.01.24 ~ 2024.03.27(TBD)
- Presenter : 강동규, 오원준, 나보영, 이민지, 이승호, 김민준, 김해문, 박지영

| Date | Paper |[s Category | Presenter | Link |
| ---- | ---- | ---- | ---- | ---- |
| 2024.01.24 | Parameter-Efficient Transfer Learning for NLP | PEFT | 나보영 | [Paper](https://arxiv.org/abs/1902.00751)</br>[Review](https://github.com/devkade/DeepSync/blob/main/Docs/sprint3/Parameter-Efficient%20Transfer%20Learning%20for%20NLP.pdf) |
| 2024.01.31 | LoRA | PEFT | 강동규 | [Paper](https://arxiv.org/abs/2106.09685)</br> [Slide](https://github.com/devkade/DeepSync/tree/main/Docs/sprint3/LoRA.pdf) |
| 2024.02.14 | Prefix-Tuning | PEFT | 박지영 | [Paper](https://arxiv.org/abs/2101.00190) </br> [Slide](https://github.com/devkade/DeepSync/blob/bc4ac2695c945d1985f974900bba35cf6727394c/Docs/sprint3/Prefix-Tuning%20Optimizing%20Continuous%20Prompts%20for%20Generation.pdf) |
| 2024.02.21 | Few-Shot Parameter-Efficient Fine-Tuning vs. In-Context Learning | PEFT | 강동규 | [Paper](https://arxiv.org/abs/2205.05638) <br>[Slide](https://github.com/devkade/DeepSync/blob/main/Docs/sprint3/Few-Shot%20PEFT%20is%20Better%20and%20Cheaper%20than%20ICL.pdf) |
| 2024.03.13 | QLoRA | PEFT | 이민지 | [Paper](https://arxiv.org/abs/2305.14314) |
| 2024.02.14 | A Survey of Quantization Methods for Efficient Neural Network Inference | Quantization | 이민지 | [Paper](https://arxiv.org/abs/2103.13630) |
| 2024.02.21 | Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference | Quantization | 김민준 | [Paper](https://arxiv.org/abs/1712.05877) </br>[Slide](https://github.com/devkade/DeepSync/tree/main/Docs/sprint3/Quantization_and_Training_of_Neural_Networks_for_Efficient_Integer-Arithmetic-Only_Inference.pdf)|
| 2024.02.14 | Mixed Precision Training | Quantization | 이민지 | [Paper](https://arxiv.org/abs/1710.03740) |
|  | Quant-Noise | Quantization |  | [Paper](https://arxiv.org/abs/2004.07320) |
| 2024.03.06 | LLM.int8() | Quantization | 김민준 | [Paper](https://arxiv.org/abs/2208.07339) |
| 2024.03.20 | GPTQ | Quantization | 나보영 | [Paper](https://arxiv.org/abs/2210.17323) |
| 2024.01.24 | Transformer 톺아보기 | Efficient Transformer | 이승호 |[Review](https://github.com/devkade/DeepSync/blob/main/Docs/sprint3/Transformer%ED%86%BA%EC%95%84%EB%B3%B4%EA%B8%B0.md)  |
| 2024.02.14 | MegatronLM | Efficient Transformer | 이승호 | [Paper](https://arxiv.org/abs/1909.08053) |
| 2024.01.24 | Swin Transformer | Efficient Transformer | 오원준 | [Paper](https://arxiv.org/abs/2103.14030)</br>[Review](https://ownogatari.xyz/posts/swin/)</br>[Slide](https://drive.google.com/file/d/1lKkg_PGO7e2EJt5hMZuLiPtA4QZJhIgo/view?usp=sharing)   |
| 2024.03.13 | zero | Efficient Transformer | 이승호 | [Paper](https://arxiv.org/abs/1910.02054) |
|  | zero infinity | Efficient Transformer |  | [Paper](https://arxiv.org/abs/2104.07857) |
| 2024.02.14 | Switch Transformer | Efficient Transformer | 오원준  | [Paper](https://arxiv.org/abs/2101.03961)</br>[Slide](https://drive.google.com/file/d/1Q7rAb1MIBOnWCkSuQRQzzvXlw7xyo4Of/view?usp=sharing) |
|  | DeepSpeed-MoE | Efficient Transformer |  | [Paper](https://arxiv.org/abs/2201.05596) |
| 2024.01.31 | Paged Attention | Efficient Transformer | 김민준 | [Paper](https://arxiv.org/abs/2309.06180)<br>[Slide](https://github.com/devkade/DeepSync/blob/main/Docs/sprint3/Paged_Attention.pdf) |
|  | FlashAttention | Efficient Transformer |  | [Paper](https://arxiv.org/abs/2205.14135) |
| 2024.02.28 | FastViT | Efficient Transformer | 박지영 | [Paper](https://arxiv.org/pdf/2303.14189.pdf)</br>[Slide](https://github.com/devkade/DeepSync/blob/main/Docs/sprint3/FastViT.pdf) |
| 2024.03.06 | Biformer | Efficient Transformer | 오원준 | [Paper](https://arxiv.org/pdf/2303.08810.pdf)<br>[Slide](https://drive.google.com/file/d/1pHEURkDF5T49DxzWHy6beX_nGiVSafut/view?usp=sharing) |
| 2024.03.13 | Knowledge Distillation: A Survey | Knowledge Distillation | 김해문 | [Paper](https://arxiv.org/abs/2006.05525) |
| 2024.02.07 | DeiT | Knowledge Distillation | 김해문 | [Paper](https://arxiv.org/abs/2012.12877)</br> [Slide](https://drive.google.com/file/d/1d6lnXw-qwoMDoWg8OdR99ZwrW4xdUmxJ/view?usp=sharing)  |
| 2024.01.31 | Generative Agents: Interactive Simulacra of Human Behavior | AI Agent | 박지영 | [Paper](https://arxiv.org/abs/2304.03442)</br>[Review](https://github.com/devkade/DeepSync/blob/main/Docs/sprint3/Generative.Agents-Interactive.Simulacra.of.Human.Behavior.pdf) |
|  | The Rise and Potential of Large Language Model Based Agents: A Survey | AI Agent |  | [Paper](https://arxiv.org/abs/2309.07864) |
| 2024.02.07 | Agents: An Open-source Framework for Autonomous Language Agents | AI Agent | 나보영 | [Paper](https://arxiv.org/abs/2309.07870)</br>[Review](https://github.com/devkade/DeepSync/blob/main/Docs/sprint3/Agents%20An%20Open-source%20Framework%20for%20Autonomous%20Language%20Agents.pdf)      |
|  | JARVIS-1: Open-World Multi-task Agents with Memory-Augmented Multimodal Language Models | AI Agent |  | [Paper](https://arxiv.org/abs/2311.05997) |
